{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2f7c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src path: /ehome/xinyi/Xinyi_GNN_aorta/src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "# 获取当前文件的绝对路径\n",
    "current_file_path = os.getcwd()\n",
    "\n",
    "# 跳出 g_unet 目录，进入 Xinyi_GNN_aorta/src\n",
    "project_root = os.path.abspath(os.path.join(current_file_path, \"..\"))\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "\n",
    "print(f\"src path: {src_path}\")\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e15a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import aorta_3D_info, Normalizer_ts,vtp2Graph_aorta\n",
    "\n",
    "# # In this code block we will derive the data information for the aorta_toy dataset.\n",
    "# raw_data_path = '/ehome/xinyi/Xinyi_GNN_aorta/data/toy_unet/raw'\n",
    "# outputpath = '/ehome/xinyi/Xinyi_GNN_aorta/data/toy_unet'\n",
    "# data_label = ['y', 'pos', 'stmdist']\n",
    "# data_information = aorta_3D_info(path=raw_data_path, labels=data_label, output_path=outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b832efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv already exists, loading data...\n",
      "data load done.\n"
     ]
    }
   ],
   "source": [
    "from utils import suk_aorta_3D_info, Normalizer_ts\n",
    "\n",
    "processed_path = '/ehome/xinyi/Xinyi_GNN_aorta/data/vessel-dataset/single_toy_unet/raw'\n",
    "outputpath     = '/ehome/xinyi/Xinyi_GNN_aorta/data/vessel-dataset/single_toy_unet'\n",
    "data_label = ['y', 'pos', \"stmdist\"]\n",
    "\n",
    "data_information = suk_aorta_3D_info(path=processed_path, labels=data_label, output_path=outputpath)\n",
    "dfs  = data_information.graph2pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b4db89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pressure</th>\n",
       "      <td>139176.950000</td>\n",
       "      <td>131830.520000</td>\n",
       "      <td>134748.730000</td>\n",
       "      <td>1287.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wssx</th>\n",
       "      <td>403.141170</td>\n",
       "      <td>-134.361650</td>\n",
       "      <td>21.726862</td>\n",
       "      <td>29.298239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wssy</th>\n",
       "      <td>225.556690</td>\n",
       "      <td>-224.741970</td>\n",
       "      <td>0.424268</td>\n",
       "      <td>20.644999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wssz</th>\n",
       "      <td>90.428570</td>\n",
       "      <td>-100.762505</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>5.544290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>6.126948</td>\n",
       "      <td>-1.918083</td>\n",
       "      <td>2.098806</td>\n",
       "      <td>2.182891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>1.982128</td>\n",
       "      <td>-2.325242</td>\n",
       "      <td>-0.062998</td>\n",
       "      <td>0.609867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.198804</td>\n",
       "      <td>-0.198806</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.119057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distance</th>\n",
       "      <td>8.815603</td>\n",
       "      <td>-0.041465</td>\n",
       "      <td>3.953391</td>\n",
       "      <td>2.390356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    max            min           mean          std\n",
       "label                                                             \n",
       "pressure  139176.950000  131830.520000  134748.730000  1287.432600\n",
       "wssx         403.141170    -134.361650      21.726862    29.298239\n",
       "wssy         225.556690    -224.741970       0.424268    20.644999\n",
       "wssz          90.428570    -100.762505      -0.004054     5.544290\n",
       "x              6.126948      -1.918083       2.098806     2.182891\n",
       "y              1.982128      -2.325242      -0.062998     0.609867\n",
       "z              0.198804      -0.198806       0.000279     0.119057\n",
       "distance       8.815603      -0.041465       3.953391     2.390356"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_information.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a4ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Extracting output and input ranges from the data information for data normalization\n",
    "# output_range = data_information.info.values[1].reshape(1, -1) # pressure!!!!!!!!!!!!!\n",
    "# input_range = data_information.info.values[4:] \n",
    "\n",
    "# output_range = output_range.astype(np.float32)\n",
    "# input_range = input_range.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def5b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extracting output ranges from the data information for data normalization\n",
    "\n",
    "output_range = data_information.info.values[0].reshape(1, -1) # pressure!!!!!!!!!!!!!\n",
    "output_range = output_range.astype(np.float32)\n",
    "\n",
    "# Step 1: 提取 x, y, z 三行的数据（索引为 4, 5, 6）\n",
    "xyz_rows = data_information.info.values[4:7].astype(np.float32)\n",
    "\n",
    "# Step 2: 提取 mean 和 std 列（索引 2 和 3）\n",
    "xyz_mean = xyz_rows[:, 2]  # shape = (3,)\n",
    "xyz_std = xyz_rows[:, 3]   # shape = (3,)\n",
    "\n",
    "# Step 3: 计算 mean 和 std 的平均值\n",
    "shared_mean = np.mean(xyz_mean)\n",
    "shared_std = np.mean(xyz_std)\n",
    "\n",
    "# Step 4: 构造新的 mean/std 参数，用于 x/y/z 和其他维度（比如 distance）拼接\n",
    "# x/y/z 使用统一的 mean/std，distance 用原来的\n",
    "distance_row = data_information.info.values[7].astype(np.float32)  # index 7 是 distance\n",
    "\n",
    "# 构造 input normalization 参数，按列顺序排列\n",
    "# shape = (2, 4) → [mean_row, std_row]\n",
    "input_mean = np.array([shared_mean, shared_mean, shared_mean, distance_row[2]])\n",
    "input_std = np.array([shared_std, shared_std, shared_std, distance_row[3]])\n",
    "input_params = np.vstack([input_mean, input_std])  # shape = (2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5566af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_normalizer = Normalizer_ts(method='ms',params=input_range[:,2:].T)\n",
    "input_normalizer = Normalizer_ts(method='ms', params=input_params)\n",
    "output_normalizer = Normalizer_ts(method=\"ms\", params=output_range[:,2:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d913eeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|██████████| 11/11 [00:08<00:00,  1.25it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from Bench_Heat.heat_sampling_suk import HeatSamplingCluster\n",
    "from dataset import Aorta_3d_Dataset\n",
    "\n",
    "# Multiscale radius graph\n",
    "ratios = [1., 0.4, 0.1]\n",
    "radii = [0.042, 0.06, 0.8] # if artery_type == 'single' \n",
    "# else [0.022, 0.04, 0.1]\n",
    "\n",
    "trans = HeatSamplingCluster(ratios, radii)\n",
    "\n",
    "\n",
    "root = '/ehome/xinyi/Xinyi_GNN_aorta/data/vessel-dataset/single_toy_unet'\n",
    "dataset = Aorta_3d_Dataset(root,'cpu', label='pressure', pre_transform=trans,input_normalizer=input_normalizer,output_normalizer=output_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7ce023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import pyvista as pv\n",
    "\n",
    "# def export_multiscale_vtp(data, prefix='aorta'):\n",
    "#     \"\"\"\n",
    "#     兼容 MultiscaleData:\n",
    "#     x=[N, *], pos=[N, 3], face=[3, F],\n",
    "#     scale{i}_sample_index, scale{i}_cluster_map, scale{i}_edge_index, ...\n",
    "#     导出：\n",
    "#       1) {prefix}_multiscale_mesh.vtp  （带每层 mask 与 cluster）\n",
    "#       2) {prefix}_scale{i}_points.vtp  （各层采样点云）\n",
    "#       3) {prefix}_multiscale_bundle.vtm（MultiBlock，方便一次性打开）\n",
    "#     \"\"\"\n",
    "#     # -------- 0) 取点坐标（pos_raw优先）\n",
    "#     points = data.pos_raw.cpu().numpy() if hasattr(data, 'pos_raw') else data.pos.cpu().numpy()\n",
    "#     N = points.shape[0]\n",
    "\n",
    "#     # -------- 1) 构造 faces（若有）\n",
    "#     faces = None\n",
    "#     if hasattr(data, 'face') and (data.face is not None):\n",
    "#         face_t = data.face\n",
    "#         if isinstance(face_t, torch.Tensor):\n",
    "#             face_np = face_t.t().contiguous().cpu().numpy()  # (F,3)\n",
    "#         else:\n",
    "#             face_np = np.asarray(face_t)\n",
    "#         if face_np.size > 0:\n",
    "#             F = face_np.shape[0]\n",
    "#             faces = np.hstack([np.full((F, 1), 3, dtype=np.int64), face_np]).ravel().astype(np.int64)\n",
    "\n",
    "#     # -------- 2) 基础网格\n",
    "#     mesh = pv.PolyData(points, faces) if faces is not None else pv.PolyData(points)\n",
    "\n",
    "#     # -------- 3) 找出可用的 scale 键（兼容 sample_index / sampling_index）\n",
    "#     keys = list(data.keys())\n",
    "#     scale_ids = []\n",
    "#     for k in keys:\n",
    "#         if k.startswith('scale') and (k.endswith('sample_index') or k.endswith('sampling_index')):\n",
    "#             # 提取数字 id\n",
    "#             sid = int(k[len('scale'):].split('_')[0])\n",
    "#             scale_ids.append(sid)\n",
    "#     scale_ids = sorted(set(scale_ids))\n",
    "\n",
    "#     print(\"Detected scales:\", scale_ids)\n",
    "\n",
    "#     # MultiBlock 容器（把完整网格 + 各层点云一起打包）\n",
    "#     mb = pv.MultiBlock()\n",
    "#     mb['full_mesh'] = mesh.copy()\n",
    "\n",
    "#     # 保存一个全局的原始点序号（调试/显示用）\n",
    "#     mesh.point_data['point_id'] = np.arange(N, dtype=np.int32)\n",
    "\n",
    "#     # -------- 4) 逐层写 mask / cluster，并导出各层点云\n",
    "#     for sid in scale_ids:\n",
    "#         # 取 index 键名（优先 sample_index）\n",
    "#         idx_key1 = f'scale{sid}_sample_index'\n",
    "#         idx_key2 = f'scale{sid}_sampling_index'\n",
    "#         if hasattr(data, idx_key1):\n",
    "#             idx_t = getattr(data, idx_key1)\n",
    "#         else:\n",
    "#             idx_t = getattr(data, idx_key2)\n",
    "\n",
    "#         # 取 cluster（若有）\n",
    "#         cluster_key = f'scale{sid}_cluster_map'\n",
    "#         cluster_arr = None\n",
    "#         if hasattr(data, cluster_key):\n",
    "#             cluster_t = getattr(data, cluster_key)\n",
    "#             cluster_arr = cluster_t.detach().cpu().numpy().astype(np.int32).ravel()\n",
    "#             if cluster_arr.shape[0] == N:\n",
    "#                 mesh.point_data[f'scale{sid}_cluster'] = cluster_arr\n",
    "#             else:\n",
    "#                 # 长度不等于 N 就不写到 full mesh 上，避免误导\n",
    "#                 pass\n",
    "\n",
    "#         # 采样索引清洗\n",
    "#         idxs = idx_t.detach().cpu().numpy().astype(np.int64).ravel()\n",
    "#         # 有些实现可能返回布尔/全量映射，这里统一转“唯一且在界内的点索引”\n",
    "#         if idxs.dtype == np.bool_ or ((idxs.ndim == 1) and np.array_equal(np.unique(idxs), np.array([0,1])) and idxs.size==N):\n",
    "#             # 万一是布尔 mask（极少见），转成索引\n",
    "#             idxs = np.where(idxs.astype(bool))[0]\n",
    "#         else:\n",
    "#             idxs = np.unique(idxs)\n",
    "#             idxs = idxs[(idxs >= 0) & (idxs < N)]\n",
    "\n",
    "#         # 写 mask（0/1），ParaView 里用 Threshold 选 0.5~1.5 即可筛到采样点\n",
    "#         mask = np.zeros((N,), dtype=np.uint8)\n",
    "#         mask[idxs] = 1\n",
    "#         mesh.point_data[f'scale{sid}_mask'] = mask\n",
    "\n",
    "#         # 导出该层点云\n",
    "#         sampled_pts = points[idxs] if idxs.size > 0 else points[:0]\n",
    "#         sampled_cloud = pv.PolyData(sampled_pts)\n",
    "#         sampled_cloud.point_data['ones'] = np.ones((sampled_pts.shape[0],), dtype=np.float32) if idxs.size > 0 else np.array([], dtype=np.float32)\n",
    "\n",
    "#         pts_path = f'{prefix}_scale{sid}_points.vtp'\n",
    "#         sampled_cloud.save(pts_path, binary=True)\n",
    "#         print(f\"[scale{sid}] sampled points: {pts_path}  (count={sampled_pts.shape[0]})\")\n",
    "\n",
    "#         # MultiBlock 增加一块\n",
    "#         mb[f'scale{sid}_points'] = sampled_cloud\n",
    "\n",
    "#     # -------- 5) 保存带多尺度属性的整网格\n",
    "#     mesh_path = f'{prefix}_multiscale_mesh.vtp'\n",
    "#     mesh.save(mesh_path, binary=True)\n",
    "#     print(f\"Saved mesh with masks/clusters: {mesh_path}\")\n",
    "\n",
    "#     # -------- 6) 保存 MultiBlock\n",
    "#     bundle_path = f'{prefix}_multiscale_bundle.vtm'\n",
    "#     mb.save(bundle_path)\n",
    "#     print(f\"Saved multiscale bundle: {bundle_path}\")\n",
    "\n",
    "# # ====== 用法：取一个样本然后导出 ======\n",
    "# data = dataset[0]  # 你的 MultiscaleData\n",
    "# export_multiscale_vtp(data, prefix='aorta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f04570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scale0_cluster_map', 'y', 'scale0_edge_index', 'scale2_sample_index', 'scale1_cluster_map', 'scale0_sample_index', 'scale2_cluster_map', 'face', 'scale2_edge_index', 'norm', 'scale1_edge_index', 'pos', 'x', 'scale1_sample_index']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_data = torch.load('/ehome/xinyi/Xinyi_GNN_aorta/data/vessel-dataset/single_toy_unet/processed/data_0.pt', weights_only=False)\n",
    "print(raw_data.keys())  # 应该至少有 'edge_index'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e34a9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiscaleData(x=[10475, 4], y=[10475], pos=[10475, 3], norm=[10475, 3], face=[3, 20946], scale0_cluster_map=[10475], scale0_edge_index=[2, 75136], scale2_sample_index=[320], scale1_cluster_map=[10475], scale0_sample_index=[10475], scale2_cluster_map=[3493], scale2_edge_index=[2, 10198], scale1_edge_index=[2, 13048], scale1_sample_index=[3493])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "mean = data.y.mean()\n",
    "std = data.y.std()\n",
    "\n",
    "# print(f\"Mean: {mean.item():.4f}, Std: {std.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d950e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.loader import DataLoader\n",
    "# from torch.utils.data import random_split\n",
    "\n",
    "# train_size = 10\n",
    "# valid_size = 1\n",
    "# test_size = 1\n",
    "# train_dataset, test_dataset= random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))\n",
    "# _, valid_dataset = random_split(train_dataset, [train_size-valid_size, valid_size], generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 6, shuffle=False)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size = 1)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "775fc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "total = len(dataset)\n",
    "train_size = int(0.9 * total)\n",
    "valid_size = int(0.1 * total)\n",
    "test_size  = total - train_size\n",
    "\n",
    "train_dataset, test_dataset= random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))\n",
    "_, valid_dataset = random_split(train_dataset, [train_size-valid_size, valid_size], generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = 1)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a4d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an iterator from the loader\n",
    "# train_iterator = iter(train_loader)\n",
    "\n",
    "# # Get the first batch\n",
    "# first_batch = next(train_iterator)\n",
    "# print(\"First batch:\")\n",
    "# print(first_batch)\n",
    "\n",
    "\n",
    "# # Get the second batch\n",
    "# second_batch = next(train_iterator)\n",
    "# print(\"\\nSecond batch:\")\n",
    "# print(second_batch)\n",
    "\n",
    "# # ... and so on until the iterator is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f885dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttGCN (2225861 trainable parameters)\n"
     ]
    }
   ],
   "source": [
    "from Bench_Heat.baseline import AttGCN\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "model = AttGCN()\n",
    "model.to('cuda')\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=0.001)\n",
    "schedule = ExponentialLR(optimizer,.999)\n",
    "\n",
    "optim_para = {\n",
    "            'epoch': 0,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss':1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30266e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "241c09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from traincopy import train, valid  # only pressure!!! 后续记得baseline改回3维\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# #start training \n",
    "# train_epoch_error = []\n",
    "# val_epoch_error = []\n",
    "# test_error = []\n",
    "# check_idx = []\n",
    "# epochs = 1000\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     train_epoch_error.append(train(model, device, train_loader, optimizer,criterion,scheduler=schedule))\n",
    "#     val_epoch_error.append(valid(model, device, valid_loader,criterion))\n",
    "#     if (epoch+1)%100==0 or epoch==0:\n",
    "#         # val_epoch_error.append(valid(model, device, valid_loader,criterion))\n",
    "#         test_error.append(valid(model, device, test_loader,criterion))\n",
    "#         check_idx.append(epoch)\n",
    "#     # if optim_para['loss'] > train_epoch_error[epoch]:\n",
    "#     #     optim_para['epoch'] = epoch\n",
    "#     #     optim_para['model_state_dict'] = model.state_dict()\n",
    "#     #     optim_para['optimizer_state_dict']= optimizer.state_dict()\n",
    "#     #     optim_para['loss'] = train_epoch_error[epoch]\n",
    "#     # check_idx.append(epoch+1)\n",
    "\n",
    "\n",
    "#     if (epoch+1)%200==0 and epoch != 0:    \n",
    "#         fig, ax = plt.subplots(figsize=(10,8))\n",
    "#         er1 = torch.stack(train_epoch_error)\n",
    "#         er2 = torch.stack(val_epoch_error)\n",
    "#         er3 = torch.stack(test_error)\n",
    "#         ax.plot(er1.detach().cpu(),color = 'C0',linestyle='solid',linewidth=1,alpha=1,label='L_train')\n",
    "#         ax.plot(er2.detach().cpu(),color = 'C1',linestyle='solid',linewidth=1,alpha=1,label='L_valid')\n",
    "#         ax.plot(check_idx,er3.detach().cpu(),color = 'C2',linestyle='solid',linewidth=1,alpha=1,label='L_test')\n",
    "#         ax.set_yscale('log')\n",
    "#         ax.set_title('train_loss_curve')\n",
    "#         ax.set_xlabel('epochs')\n",
    "#         ax.set_ylabel('loss')\n",
    "#         ax.legend()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29d37193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:23<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from train import train, valid\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ✅ checkpoint 路径\n",
    "ckpt_dir = 'checkpoints_suk_toy_no_norm'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# ✅ 图像保存路径\n",
    "fig_dir = 'figures_suk_toy_no_norm'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# ✅ 初始化日志\n",
    "train_epoch_error = []\n",
    "val_epoch_error = []\n",
    "test_epoch_error = []\n",
    "check_idx = []\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# ✅ 开始训练\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion, scheduler=schedule)\n",
    "    train_epoch_error.append(train_loss)\n",
    "\n",
    "    if (epoch  % 100) == 0:\n",
    "        val_loss = valid(model, device, valid_loader, criterion)\n",
    "        test_loss = valid(model, device, test_loader, criterion)\n",
    "\n",
    "        val_epoch_error.append(val_loss)\n",
    "        test_epoch_error.append(test_loss)\n",
    "        check_idx.append(epoch)\n",
    "\n",
    "        # ✅ 保存 checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'test_loss': test_loss\n",
    "        }, os.path.join(ckpt_dir, f'checkpoint_epoch_{epoch}.pt'))\n",
    "\n",
    "        # ✅ 画图 1：训练损失\n",
    "        fig1, ax1 = plt.subplots(figsize=(8, 5))\n",
    "        ax1.plot(\n",
    "            range(len(train_epoch_error)),\n",
    "            torch.stack(train_epoch_error).detach().cpu().numpy(),\n",
    "            color='C0', label='Train Loss'\n",
    "        )\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_title('Train Loss Curve')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        fig1.tight_layout()\n",
    "        fig1.savefig(os.path.join(fig_dir, f\"train_loss_epoch_{epoch}.png\"))\n",
    "        plt.close(fig1)\n",
    "\n",
    "        # ✅ 画图 2：Val/Test 损失（每 100 epoch）\n",
    "        fig2, ax2 = plt.subplots(figsize=(8, 5))\n",
    "        ax2.plot(\n",
    "            check_idx,\n",
    "            torch.stack(val_epoch_error).detach().cpu().numpy(),\n",
    "            color='C1', label='Validation Loss'\n",
    "        )\n",
    "        ax2.plot(\n",
    "            check_idx,\n",
    "            torch.stack(test_epoch_error).detach().cpu().numpy(),\n",
    "            color='C2', label='Test Loss'\n",
    "        )\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_title('Validation & Test Loss (every 100 epochs)')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        fig2.tight_layout()\n",
    "        fig2.savefig(os.path.join(fig_dir, f\"val_test_loss_epoch_{epoch}.png\"))\n",
    "        plt.close(fig2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b078fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pyvista as pv\n",
    "# import matplotlib.pyplot as plt\n",
    "# from os.path import join as osj\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def generate_output(\n",
    "#     model,\n",
    "#     device,\n",
    "#     output_loader,\n",
    "#     raw_mesh_path,\n",
    "#     tag,\n",
    "#     save_mesh_path,\n",
    "#     mesh_name=\"mesh\",\n",
    "#     output_normalizer=None,\n",
    "# ):\n",
    "#     for _, batch in enumerate(tqdm(output_loader)):\n",
    "#         batch = batch.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             label = batch.y\n",
    "#             output = model(batch)\n",
    "#             label_denorm = output_normalizer.denormalize(label)  # shape is <N,1>\n",
    "#             output_denorm = output_normalizer.denormalize(output)  # shape is <N,1>\n",
    "#             idx = (batch.idx)[0]\n",
    "\n",
    "#             mesh = pv.read(osj(raw_mesh_path, \"aorta00{:d}.vtp\".format(idx)))\n",
    "#             mesh.point_data[tag + \"_prediction\"] = output_denorm.detach().cpu()\n",
    "#             mesh.point_data[tag + \"_label\"] = label_denorm.detach().cpu()\n",
    "#             mesh.point_data[tag + \"_error\"] = (\n",
    "#                 mesh.point_data[tag + \"_prediction\"] - mesh.point_data[tag + \"_label\"]\n",
    "#             )\n",
    "#             mesh.save(osj(save_mesh_path, mesh_name + \"_{:d}.vtp\".format(idx)))\n",
    "# generate_output(model_opt,'cpu',valid_loader,'aorta/raw','wss','results/meshes','mesh_wss_50',output_normalizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-gatr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
