{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2f7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import time as time\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# current_file_path = os.getcwd()\n",
    "# src_path = os.path.abspath(os.path.join(current_file_path, \"..\", \"src\"))\n",
    "# print(f\"src path: {src_path}\")\n",
    "# sys.path.insert(0, src_path)\n",
    "\n",
    "# import torch\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70568a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src path: /ehome/xinyi/Xinyi_GNN_aorta/src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Ëé∑ÂèñÂΩìÂâçÊñá‰ª∂ÁöÑÁªùÂØπË∑ØÂæÑ\n",
    "current_file_path = os.getcwd()\n",
    "\n",
    "# Ë∑≥Âá∫ g_unet ÁõÆÂΩïÔºåËøõÂÖ• Xinyi_GNN_aorta/src\n",
    "project_root = os.path.abspath(os.path.join(current_file_path, \"..\"))\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "\n",
    "print(f\"src path: {src_path}\")\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e15a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import aorta_3D_info, Normalizer_ts,vtp2Graph_aorta\n",
    "\n",
    "# # In this code block we will derive the data information for the aorta_toy dataset.\n",
    "# raw_data_path = '/ehome/xinyi/Xinyi_GNN_aorta/data/toy_trans/raw'\n",
    "# outputpath = '/ehome/xinyi/Xinyi_GNN_aorta/data/toy_trans'\n",
    "# data_label = ['y', 'pos', 'stmdist']\n",
    "# data_information = aorta_3D_info(path=raw_data_path, labels=data_label, output_path=outputpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a83e230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 686.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data generation done, saved in directory:/ehome/xinyi/Xinyi_GNN_aorta/data/toy_trans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import aorta_3D_info, Normalizer_ts\n",
    "\n",
    "processed_path = '/ehome/xinyi/Xinyi_GNN_aorta/data/toy_trans/raw'\n",
    "outputpath     = '/ehome/xinyi/Xinyi_GNN_aorta/data/toy_trans'\n",
    "data_label = ['y', 'pos', \"stmdist\"]\n",
    "\n",
    "data_information = aorta_3D_info(path=processed_path, labels=data_label, output_path=outputpath)\n",
    "dfs  = data_information.graph2pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c375595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pressure</th>\n",
       "      <td>3.398470</td>\n",
       "      <td>-7.650037</td>\n",
       "      <td>1.278087</td>\n",
       "      <td>1.304616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wssx</th>\n",
       "      <td>0.021710</td>\n",
       "      <td>-0.046168</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>0.006476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wssy</th>\n",
       "      <td>0.010297</td>\n",
       "      <td>-0.074118</td>\n",
       "      <td>-0.008381</td>\n",
       "      <td>0.009542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wssz</th>\n",
       "      <td>0.079113</td>\n",
       "      <td>-0.036755</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.017247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>38.824947</td>\n",
       "      <td>-17.211851</td>\n",
       "      <td>15.181118</td>\n",
       "      <td>13.173093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>46.847961</td>\n",
       "      <td>-48.988758</td>\n",
       "      <td>-3.707316</td>\n",
       "      <td>24.354790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>82.896675</td>\n",
       "      <td>-5.897548</td>\n",
       "      <td>42.835144</td>\n",
       "      <td>18.339022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distance</th>\n",
       "      <td>177.199799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.495743</td>\n",
       "      <td>45.188190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 max        min       mean        std\n",
       "label                                                \n",
       "pressure    3.398470  -7.650037   1.278087   1.304616\n",
       "wssx        0.021710  -0.046168  -0.002896   0.006476\n",
       "wssy        0.010297  -0.074118  -0.008381   0.009542\n",
       "wssz        0.079113  -0.036755   0.006116   0.017247\n",
       "x          38.824947 -17.211851  15.181118  13.173093\n",
       "y          46.847961 -48.988758  -3.707316  24.354790\n",
       "z          82.896675  -5.897548  42.835144  18.339022\n",
       "distance  177.199799   0.000000  67.495743  45.188190"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_information.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d640020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# # ‚úÖ Ê≠£Á°Æ‰ΩøÁî®‰ªé data_information ÊèêÂèñÁöÑ mean/std\n",
    "# output_range = data_information.info.values[1].astype(np.float32)\n",
    "# pressure_mean = output_range[2]  # e.g. 1.5284\n",
    "# pressure_std = output_range[3]   # e.g. 2.1039\n",
    "\n",
    "# print(\"‚úÖ Using global mean/std from data_information:\")\n",
    "# print(f\"Mean = {pressure_mean:.4f}, Std = {pressure_std:.4f}\")\n",
    "\n",
    "# output_normalizer = Normalizer_ts(method=\"ms\", params=np.array([[pressure_mean], [pressure_std]]))\n",
    "\n",
    "# raw_paths = ['/ehome/xinyi/Xinyi_GNN_aorta/data/toy_trans/raw/aorta0000.vtp']\n",
    "\n",
    "# for raw_path in tqdm.tqdm(raw_paths):\n",
    "#     data, _ = vtp2Graph_aorta(raw_path)\n",
    "\n",
    "#     print(f\"üìÇ File: {raw_path}\")\n",
    "#     print(\"Before normalization:\")\n",
    "#     print(f\"  min = {data.y[:, 0].min().item():.4f}\")\n",
    "#     print(f\"  max = {data.y[:, 0].max().item():.4f}\")\n",
    "#     print(f\"  mean = {data.y[:, 0].mean().item():.4f}\")\n",
    "#     print(f\"  std  = {data.y[:, 0].std().item():.4f}\")\n",
    "\n",
    "#     data.y = output_normalizer.normalize(data.y[:, 0])\n",
    "\n",
    "#     print(\"After normalization:\")\n",
    "#     print(f\"  min = {data.y.min().item():.4f}\")\n",
    "#     print(f\"  max = {data.y.max().item():.4f}\")\n",
    "#     print(f\"  mean = {data.y.mean().item():.4f}\")\n",
    "#     print(f\"  std  = {data.y.std().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a4ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Extracting output and input ranges from the data information for data normalization\n",
    "# output_range = data_information.info.values[1].reshape(1, -1) # pressure!!!!!!!!!!!!!\n",
    "# input_range = data_information.info.values[4:] \n",
    "\n",
    "\n",
    "# output_range = output_range.astype(np.float32)\n",
    "# input_range = input_range.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0218ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extracting output ranges from the data information for data normalization\n",
    "\n",
    "output_range = data_information.info.values[0].reshape(1, -1) # pressure!!!!!!!!!!!!!\n",
    "output_range = output_range.astype(np.float32)\n",
    "\n",
    "# Step 1: ÊèêÂèñ x, y, z ‰∏âË°åÁöÑÊï∞ÊçÆÔºàÁ¥¢Âºï‰∏∫ 4, 5, 6Ôºâ\n",
    "xyz_rows = data_information.info.values[4:7].astype(np.float32)\n",
    "\n",
    "# Step 2: ÊèêÂèñ mean Âíå std ÂàóÔºàÁ¥¢Âºï 2 Âíå 3Ôºâ\n",
    "xyz_mean = xyz_rows[:, 2]  # shape = (3,)\n",
    "xyz_std = xyz_rows[:, 3]   # shape = (3,)\n",
    "\n",
    "# Step 3: ËÆ°ÁÆó mean Âíå std ÁöÑÂπ≥ÂùáÂÄº\n",
    "shared_mean = np.mean(xyz_mean)\n",
    "shared_std = np.mean(xyz_std)\n",
    "\n",
    "# Step 4: ÊûÑÈÄ†Êñ∞ÁöÑ mean/std ÂèÇÊï∞ÔºåÁî®‰∫é x/y/z ÂíåÂÖ∂‰ªñÁª¥Â∫¶ÔºàÊØîÂ¶Ç distanceÔºâÊãºÊé•\n",
    "# x/y/z ‰ΩøÁî®Áªü‰∏ÄÁöÑ mean/stdÔºådistance Áî®ÂéüÊù•ÁöÑ\n",
    "distance_row = data_information.info.values[7].astype(np.float32)  # index 7 ÊòØ distance\n",
    "\n",
    "# ÊûÑÈÄ† input normalization ÂèÇÊï∞ÔºåÊåâÂàóÈ°∫Â∫èÊéíÂàó\n",
    "# shape = (2, 4) ‚Üí [mean_row, std_row]\n",
    "input_mean = np.array([shared_mean, shared_mean, shared_mean, distance_row[2]])\n",
    "input_std = np.array([shared_std, shared_std, shared_std, distance_row[3]])\n",
    "input_params = np.vstack([input_mean, input_std])  # shape = (2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df5566af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_normalizer = Normalizer_ts(method='ms',params=input_range[:,2:].T)\n",
    "input_normalizer = Normalizer_ts(method='ms', params=input_params)\n",
    "output_normalizer = Normalizer_ts(method=\"ms\", params=output_range[:,2:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a623de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_cluster import fps, knn\n",
    "# @torch.no_grad()\n",
    "# def positional_encoding(data):\n",
    "\n",
    "#     vectors_to = {key: data.pos[value.long()] - data.pos for key, value in compute_nearest_boundary_vertex(data).items()}\n",
    "#     distances_to = {key: torch.linalg.norm(value, dim=-1, keepdim=True) for key, value in vectors_to.items()}\n",
    "\n",
    "#     data.x = torch.cat((\n",
    "#         vectors_to['inlet'] / torch.clamp(distances_to['inlet'], min=1e-16),\n",
    "#         vectors_to['lumen_wall'] / torch.clamp(distances_to['lumen_wall'], min=1e-16),\n",
    "#         vectors_to['outlets'] / torch.clamp(distances_to['outlets'], min=1e-16),\n",
    "#         distances_to['inlet'],\n",
    "#         distances_to['lumen_wall'],\n",
    "#         distances_to['outlets']\n",
    "#     ), dim=1)\n",
    "\n",
    "#     return data\n",
    "\n",
    "# def compute_nearest_boundary_vertex(data):\n",
    "#     index_dict = {}\n",
    "\n",
    "#     for key in ('inlet', 'lumen_wall', 'outlets'):\n",
    "#         index_dict[key] = data[f'{key}_index'][knn(data.pos[data[f'{key}_index'].long()], data.pos, k=1)[1].long()]\n",
    "\n",
    "#     return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32eb9177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 18.58it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from dataset_copy import Aorta_3d_Dataset\n",
    "from lab_gatr.transforms import PointCloudPoolingScales\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# 1. ÂÆö‰πâÈááÊ†∑ÊØî‰æãÂíåÊèíÂÄºÁÆÄÂçïÂΩ¢\n",
    "sampling_ratios = (0.1,)\n",
    "interpolation_simplex = 'triangle'\n",
    "\n",
    "# 2. Ê≠£Á°ÆÂú∞ÂàõÂª∫ PointCloudPoolingScales ÂÆû‰æã\n",
    "#    ‰º†ÂÖ• rel_sampling_ratios Âíå interp_simplex ÂèÇÊï∞\n",
    "transforms = PointCloudPoolingScales(rel_sampling_ratios=sampling_ratios, interp_simplex=interpolation_simplex)\n",
    "\n",
    "root = '/ehome/xinyi/Xinyi_GNN_aorta/data/toy_trans'\n",
    "dataset = Aorta_3d_Dataset(root,'cpu', label='pressure', pre_transform=transforms,input_normalizer=input_normalizer,output_normalizer=output_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca967ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import pyvista as pv\n",
    "\n",
    "# # Âèñ‰∏Ä‰∏™Ê†∑Êú¨\n",
    "# data = dataset[0]\n",
    "\n",
    "# # -------- 1) ÈÄâÊã©Áî®‰∫éÂèØËßÜÂåñÁöÑÂùêÊ†áÔºàÁúüÂÆûÂá†‰ΩïÔºâ---------\n",
    "# # Ëã•‰Ω†Âú®process‰∏≠‰øùÂ≠ò‰∫Üpos_rawÔºåÂ∞±‰ºòÂÖà‰ΩøÁî®ÂÆÉÔºõÂê¶ÂàôÈÄÄÂõûpos\n",
    "# points = data.pos_raw.cpu().numpy() if hasattr(data, 'pos_raw') else data.pos.cpu().numpy()\n",
    "# N = points.shape[0]\n",
    "\n",
    "# # -------- 2) ÊûÑÈÄ†‰∏âËßíÈù¢ÁâáÔºàËã•ÊúâÔºâ---------\n",
    "# faces = None\n",
    "# if hasattr(data, 'face') and data.face is not None:\n",
    "#     # data.face ÈÄöÂ∏∏ÊòØÂΩ¢Áä∂ [3, F] ÁöÑLongTensorÔºàÊØèÂàó‰∏Ä‰∏™‰∏âËßíÂΩ¢Ôºâ\n",
    "#     face_np = data.face.t().cpu().numpy()  # (F, 3)\n",
    "#     F = face_np.shape[0]\n",
    "#     # VTK facesÈúÄË¶Å [3, i0, i1, i2, 3, j0, j1, j2, ...]\n",
    "#     faces = np.hstack([np.full((F, 1), 3, dtype=np.int64), face_np]).ravel()\n",
    "\n",
    "# # -------- 3) ÊûÑÂª∫ÂéüÂßãÁΩëÊ†º PolyData ---------\n",
    "# if faces is not None:\n",
    "#     mesh = pv.PolyData(points, faces)\n",
    "# else:\n",
    "#     # Ê≤°ÊúâÈù¢Áâá‰πüÂèØ‰ª•Âè™‰øùÂ≠òÁÇπ‰∫ëÔºàParaViewÈáå‰ª• Points Ê®°ÂºèÊòæÁ§∫Ôºâ\n",
    "#     mesh = pv.PolyData(points)\n",
    "\n",
    "# # -------- 4) ÂÜôÂÖ•ÂêÑÂ∞∫Â∫¶ÁöÑÈááÊ†∑Ê†áËÆ∞Âà∞ÁÇπÂ±ûÊÄß ---------\n",
    "# # Ëá™Âä®ÂèëÁé∞ÊâÄÊúâ scale{i}_sampling_index\n",
    "# scale_keys = sorted([k for k in data.keys() if k.endswith('sampling_index')])\n",
    "# print(\"Found sampling keys:\", scale_keys)\n",
    "\n",
    "# for sk in scale_keys:\n",
    "#     # sk ÂΩ¢Â¶Ç 'scale0_sampling_index', 'scale1_sampling_index', ...\n",
    "#     idxs = data[sk].cpu().numpy().astype(np.int64)   # (M,)\n",
    "#     mask = np.zeros((N,), dtype=np.int8)\n",
    "#     mask[idxs] = 1\n",
    "#     # ÁÇπÂ±ûÊÄßÂêçÔºö‰æãÂ¶Ç 'scale0_mask'\n",
    "#     attr_name = sk.replace('_sampling_index', '_mask')\n",
    "#     mesh.point_data[attr_name] = mask\n",
    "\n",
    "#     # ÂêåÊó∂ÂØºÂá∫ËØ•Â∞∫Â∫¶ÁöÑÁÇπ‰∫ëÔºàÂè™ÂåÖÂê´Ë¢´ÈááÊ†∑ÁöÑÁÇπÔºâÔºå‰æø‰∫éÂçïÁã¨Âè†Âä†Êü•Áúã\n",
    "#     sampled_pts = points[idxs]\n",
    "#     sampled_cloud = pv.PolyData(sampled_pts)\n",
    "#     # ÂèØÈÄâÔºöÁªôÁÇπ‰∫ë‰∏Ä‰∏™Â∏∏ÈáèÊ†áÈáèÔºåÊñπ‰æø ParaView ÁùÄËâ≤\n",
    "#     sampled_cloud.point_data['ones'] = np.ones((sampled_pts.shape[0],), dtype=np.float32)\n",
    "\n",
    "#     # ‰øùÂ≠òÁã¨Á´ãÁÇπ‰∫ë\n",
    "#     out_points_vtp = f'aorta_{sk}_points.vtp'\n",
    "#     sampled_cloud.save(out_points_vtp)\n",
    "#     print(f\"Saved sampled points: {out_points_vtp}\")\n",
    "\n",
    "# # -------- 5) ‰øùÂ≠òÂ∏¶Â§öÂ∞∫Â∫¶maskÁöÑÂÆåÊï¥ÁΩëÊ†º ---------\n",
    "# out_mesh_vtp = 'aorta_with_scales_mask.vtp'\n",
    "# mesh.save(out_mesh_vtp)\n",
    "# print(f\"Saved mesh with masks: {out_mesh_vtp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701465d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.1721, Std: 0.7942\n"
     ]
    }
   ],
   "source": [
    "# print(dataset[0])\n",
    "data = dataset[0].to(device)\n",
    "mean = data.y.mean()\n",
    "std = data.y.std()\n",
    "\n",
    "print(f\"Mean: {mean.item():.4f}, Std: {std.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec97807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "total = len(dataset)\n",
    "train_size = int(0.9 * total)\n",
    "valid_size = int(0.1 * total)\n",
    "test_size  = total - train_size\n",
    "\n",
    "train_dataset, test_dataset= random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))\n",
    "_, valid_dataset = random_split(train_dataset, [train_size-valid_size, valid_size], generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = 1)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31ce68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lab_gatr.nn.gnn import PointCloudPooling, pool\n",
    "# my_mlp = torch.nn.Linear(6, 128)\n",
    "# my_pool = PointCloudPooling(my_mlp)\n",
    "# pool(my_pool, dataset[0].pos, dataset[0].pos, dataset[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2084a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "# from torch_geometric.loader import DataLoader\n",
    "\n",
    "# # ‚Äî‚Äî ÊâìÂç∞Êï∞ÊçÆÈõÜÂ§ßÂ∞è ‚Äî‚Äî \n",
    "# print(\"Total dataset size:\", len(dataset))\n",
    "\n",
    "# # ‚Äî‚Äî Âõ∫ÂÆöÈöèÊú∫ÁßçÂ≠ê ‚Äî‚Äî \n",
    "# manual_seed = 42\n",
    "# generator = torch.Generator().manual_seed(manual_seed)\n",
    "\n",
    "# # ‚Äî‚Äî Á¨¨‰∏ÄÊ≠•Ôºö10:1 ÂàÜÂá∫ test ‚Äî‚Äî \n",
    "# total_size = len(dataset)\n",
    "# test_size = int(1/11 * total_size)\n",
    "# train_val_size = total_size - test_size\n",
    "\n",
    "# train_val_dataset, test_dataset = random_split(\n",
    "#     dataset,\n",
    "#     [train_val_size, test_size],\n",
    "#     generator=generator\n",
    "# )\n",
    "\n",
    "# # ‚Äî‚Äî Á¨¨‰∫åÊ≠•Ôºö‰ªé train_val ‰∏≠ÂÜçÂàÜ 1/10 Áªô valid ‚Äî‚Äî \n",
    "# valid_size = int(train_val_size * 0.1)\n",
    "# train_size = train_val_size - valid_size\n",
    "\n",
    "# train_dataset, valid_dataset = random_split(\n",
    "#     train_val_dataset,\n",
    "#     [train_size, valid_size],\n",
    "#     generator=generator\n",
    "# )\n",
    "\n",
    "# # ‚Äî‚Äî ÊûÑÈÄ† DataLoader ‚Äî‚Äî \n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "# test_loader  = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# # ‚Äî‚Äî ÊâìÂç∞ÊúÄÁªàÂ§ßÂ∞è ‚Äî‚Äî \n",
    "# print(f\"Train: {len(train_dataset)}, Valid: {len(valid_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d93385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lab_gatr import LaBGATr  # ‚Üê Ê†πÊçÆ‰Ω†ÁöÑÂÆûÈôÖÊñá‰ª∂ÂêçÊîπ\n",
    "# from gatr.interface import embed_oriented_plane\n",
    "# from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# # ÂÅáËÆæ‰Ω†Â∑≤Âä†ËΩΩÂ•Ω datasetÔºà‰æãÂ¶Ç DataLoader ÈáåÊãø‰∏Ä‰∏™ sampleÔºâ\n",
    "# sample = dataset[0]\n",
    "\n",
    "# # ÂáÜÂ§áÂá†‰Ωï‰ª£Êï∞Êé•Âè£\n",
    "# geometric_algebra_interface = embed_oriented_plane(\n",
    "#     sample.x,\n",
    "#     num_input_scalars=sample.x.size(-1),  # Ëá™Âä®Êé®Êñ≠ËæìÂÖ•Ê†áÈáèÊï∞\n",
    "#     num_output_scalars=sample.y.size(-1)  # Ëá™Âä®Êé®Êñ≠ËæìÂá∫Ê†áÈáèÊï∞\n",
    "# )\n",
    "\n",
    "# # ÈÄöÈÅìÂèÇÊï∞Ëá™Âä®Á°ÆÂÆö\n",
    "# in_ch = sample.x.size(-1)\n",
    "# out_ch = sample.y.size(-1)\n",
    "\n",
    "# # ÂàùÂßãÂåñ LaBGATr Ê®°Âûã\n",
    "# model = LaBGATr(\n",
    "#     geometric_algebra_interface=geometric_algebra_interface,\n",
    "#     d_model=64,\n",
    "#     num_blocks=4,\n",
    "#     num_attn_heads=4,\n",
    "#     num_latent_channels=64,\n",
    "#     use_class_token=False,\n",
    "#     dropout_probability=0.1,\n",
    "#     pooling_mode='cross_attention'  # Êàñ 'message_passing'\n",
    "# )\n",
    "\n",
    "# model = model.to('cuda')  # ‰ΩøÁî® GPU\n",
    "\n",
    "# # ÊçüÂ§±ÂáΩÊï∞‰∏é‰ºòÂåñÂô®\n",
    "# lr = 0.0001\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "# schedule = ExponentialLR(optimizer, gamma=0.999)\n",
    "\n",
    "# # ËÆ≠ÁªÉÁä∂ÊÄÅ‰øùÂ≠òÂ≠óÂÖ∏\n",
    "# optim_para = {\n",
    "#     'epoch': 0,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'loss': 1,\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60fcf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gatr.interface import embed_point, embed_oriented_plane, extract_oriented_plane\n",
    "# class GeometricAlgebraInterface:\n",
    "#     num_input_channels = 1 + 3  # vertex positions plus positional encoding vectors\n",
    "#     num_output_channels = 1\n",
    "\n",
    "#     num_input_scalars = 16  # positional encoding sclars\n",
    "#     num_output_scalars = 4\n",
    "\n",
    "#     @staticmethod\n",
    "#     @torch.no_grad()\n",
    "#     def embed(data):\n",
    "\n",
    "#         multivectors = torch.cat((\n",
    "#             embed_point(data.pos).view(-1, 1, 16),\n",
    "#             *(embed_oriented_plane(data.x[:, slice(i * 3, i * 3 + 3)], data.pos).view(-1, 1, 16) for i in range(3))\n",
    "#         ), dim=1)\n",
    "#         scalars = data.x[:, 9:]\n",
    "\n",
    "#         return multivectors, scalars\n",
    "\n",
    "#     @staticmethod\n",
    "#     def dislodge(multivectors, scalars):\n",
    "#         return extract_oriented_plane(multivectors).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7805a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gatr.interface import embed_oriented_plane, extract_translation\n",
    "\n",
    "class GeometricAlgebraInterface:\n",
    "    num_input_channels = num_output_channels = 1\n",
    "    num_input_scalars = num_output_scalars = 1\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def embed(data):\n",
    "\n",
    "        multivectors = embed_oriented_plane(normal=data.orientation, position=data.pos).view(-1, 1, 16)\n",
    "        scalars = data.scalar_feature.view(-1, 1)\n",
    "\n",
    "        return multivectors, scalars\n",
    "\n",
    "    @staticmethod\n",
    "    def dislodge(multivectors, scalars):\n",
    "        return extract_translation(multivectors).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e257c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gatr.interface import embed_point, embed_oriented_plane, extract_oriented_plane\n",
    "\n",
    "# class GeometricAlgebraInterface:\n",
    "#     num_input_channels = 1 + 3  # vertex positions plus positional encoding vectors\n",
    "#     num_output_channels = 1\n",
    "\n",
    "#     num_input_scalars = 3  # positional encoding sclars\n",
    "#     num_output_scalars = 1\n",
    "\n",
    "#     @staticmethod\n",
    "#     @torch.no_grad()\n",
    "#     def embed(data):\n",
    "\n",
    "#         multivectors = torch.cat((\n",
    "#             embed_point(data.pos).view(-1, 1, 16),\n",
    "#             *(embed_oriented_plane(data.x[:, slice(i * 3, i * 3 + 3)], data.pos).view(-1, 1, 16) for i in range(3))\n",
    "#         ), dim=1)\n",
    "#         scalars = data.x[:, 9:]\n",
    "\n",
    "#         return multivectors, scalars\n",
    "\n",
    "#     @staticmethod\n",
    "#     def dislodge(multivectors, scalars):\n",
    "#         return extract_oriented_plane(multivectors).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "307cfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, '../lab_gatr')\n",
    "# from nn.gnn import PointCloudPooling, pool  \n",
    "\n",
    "# my_mlp = torch.nn.Linear(6, 128)\n",
    "# my_pool = PointCloudPooling(my_mlp)\n",
    "# pool(my_pool, my_dataset[0].pos, my_dataset[0].pos, my_dataset[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f885dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaB-GATr (2121104 parameters)\n"
     ]
    }
   ],
   "source": [
    "from lab_gatr.models.lab_gatr import LaBGATr\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "geo_intf = GeometricAlgebraInterface  # ‰Ω†ÂÆö‰πâÁöÑÈÇ£‰∏™ class\n",
    "\n",
    "model = LaBGATr(\n",
    "    geometric_algebra_interface=geo_intf,\n",
    "    d_model=64,\n",
    "    num_blocks=4,\n",
    "    num_attn_heads=4,\n",
    "    num_latent_channels=48,   # ÂøÖÈ°ªÂÅ∂Êï∞\n",
    "    use_class_token=False,\n",
    "    dropout_probability=0.1,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=0.001)\n",
    "schedule = ExponentialLR(optimizer,.999)\n",
    "\n",
    "optim_para = {\n",
    "            'epoch': 0,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss':1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d89774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     # Âèñ‰∏ÄÂ∞èÊâπÊ†∑Êú¨\n",
    "#     sample = next(iter(train_loader))\n",
    "#     # print(sample)\n",
    "#     # sample = dataset[0].to(device)\n",
    "#     out = model(sample.to(device))\n",
    "#     print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5570aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, '/ehome/xinyi/Suk/lab-gatr-test/src')\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from train import train, valid\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# #start training \n",
    "# train_epoch_error = []\n",
    "# val_epoch_error = []\n",
    "# test_error = []\n",
    "# check_idx = []\n",
    "# epochs = 1000\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     train_epoch_error.append(train(model, device, train_loader, optimizer,criterion,scheduler=schedule))\n",
    "#     val_epoch_error.append(valid(model, device, valid_loader,criterion))\n",
    "#     if (epoch+1)%100==0 or epoch==0:\n",
    "#         # val_epoch_error.append(valid(model, device, valid_loader,criterion))\n",
    "#         test_error.append(valid(model, device, test_loader,criterion))\n",
    "#         check_idx.append(epoch)\n",
    "#     # if optim_para['loss'] > train_epoch_error[epoch]:\n",
    "#     #     optim_para['epoch'] = epoch\n",
    "#     #     optim_para['model_state_dict'] = model.state_dict()\n",
    "#     #     optim_para['optimizer_state_dict']= optimizer.state_dict()\n",
    "#     #     optim_para['loss'] = train_epoch_error[epoch]\n",
    "#     # check_idx.append(epoch+1)\n",
    "\n",
    "\n",
    "#     if (epoch+1)%100==0 and epoch != 0:    \n",
    "#         fig, ax = plt.subplots(figsize=(10,8))\n",
    "#         er1 = torch.stack(train_epoch_error)\n",
    "#         er2 = torch.stack(val_epoch_error)\n",
    "#         er3 = torch.stack(test_error)\n",
    "#         ax.plot(er1.detach().cpu(),color = 'C0',linestyle='solid',linewidth=1,alpha=1,label='L_train')\n",
    "#         ax.plot(er2.detach().cpu(),color = 'C1',linestyle='solid',linewidth=1,alpha=1,label='L_valid')\n",
    "#         ax.plot(check_idx,er3.detach().cpu(),color = 'C2',linestyle='solid',linewidth=1,alpha=1,label='L_test')\n",
    "#         ax.set_yscale('log')\n",
    "#         ax.set_title('train_loss_curve')\n",
    "#         ax.set_xlabel('epochs')\n",
    "#         ax.set_ylabel('loss')\n",
    "#         ax.legend()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25686065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_2416088/803710104.py:18: GATrDeprecationWarning: The function \"extract_translation\" is deprecated, because it is not equivariant.\n",
      "  return extract_translation(multivectors).squeeze()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [06:27<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from train import train, valid\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ‚úÖ checkpoint Ë∑ØÂæÑ\n",
    "ckpt_dir = 'checkpoints_toy_vs_sgnn'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# ‚úÖ ÂõæÂÉè‰øùÂ≠òË∑ØÂæÑ\n",
    "fig_dir = 'figures_toy_vs_sgnn'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# ‚úÖ ÂàùÂßãÂåñÊó•Âøó\n",
    "train_epoch_error = []\n",
    "val_epoch_error = []\n",
    "test_epoch_error = []\n",
    "check_idx = []\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# ‚úÖ ÂºÄÂßãËÆ≠ÁªÉ\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion, scheduler=schedule)\n",
    "    train_epoch_error.append(train_loss)\n",
    "\n",
    "    if (epoch  % 100) == 0:\n",
    "        val_loss = valid(model, device, valid_loader, criterion)\n",
    "        test_loss = valid(model, device, test_loader, criterion)\n",
    "\n",
    "        val_epoch_error.append(val_loss)\n",
    "        test_epoch_error.append(test_loss)\n",
    "        check_idx.append(epoch)\n",
    "\n",
    "        # ‚úÖ ‰øùÂ≠ò checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'test_loss': test_loss\n",
    "        }, os.path.join(ckpt_dir, f'checkpoint_epoch_{epoch}.pt'))\n",
    "\n",
    "        # ‚úÖ ÁîªÂõæ 1ÔºöËÆ≠ÁªÉÊçüÂ§±\n",
    "        fig1, ax1 = plt.subplots(figsize=(8, 5))\n",
    "        ax1.plot(\n",
    "            range(len(train_epoch_error)),\n",
    "            torch.stack(train_epoch_error).detach().cpu().numpy(),\n",
    "            color='C0', label='Train Loss'\n",
    "        )\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.set_title('Train Loss Curve')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        fig1.tight_layout()\n",
    "        fig1.savefig(os.path.join(fig_dir, f\"train_loss_epoch_{epoch}.png\"))\n",
    "        plt.close(fig1)\n",
    "\n",
    "        # ‚úÖ ÁîªÂõæ 2ÔºöVal/Test ÊçüÂ§±ÔºàÊØè 100 epochÔºâ\n",
    "        fig2, ax2 = plt.subplots(figsize=(8, 5))\n",
    "        ax2.plot(\n",
    "            check_idx,\n",
    "            torch.stack(val_epoch_error).detach().cpu().numpy(),\n",
    "            color='C1', label='Validation Loss'\n",
    "        )\n",
    "        ax2.plot(\n",
    "            check_idx,\n",
    "            torch.stack(test_epoch_error).detach().cpu().numpy(),\n",
    "            color='C2', label='Test Loss'\n",
    "        )\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_title('Validation & Test Loss (every 100 epochs)')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        fig2.tight_layout()\n",
    "        fig2.savefig(os.path.join(fig_dir, f\"val_test_loss_epoch_{epoch}.png\"))\n",
    "        plt.close(fig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "131a1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from train import train, valid\n",
    "\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # ÂàùÂßãÂåñÂàóË°®\n",
    "# train_epoch_error = []\n",
    "# val_epoch_error   = []\n",
    "# test_epoch_error  = []\n",
    "\n",
    "# train_check_idx = []          # ÊØèËΩÆ train loss ÁöÑÊ®™ËΩ¥\n",
    "# valid_check_idx = []          # ÊØè 200 ËΩÆ val loss/test loss ÁöÑÊ®™ËΩ¥\n",
    "\n",
    "# epochs = 1000  # ÂÅáËÆæËÆ≠ÁªÉ 1000 ËΩÆ\n",
    "# interval = 200  # ÊØè 200 ËΩÆËØÑ‰º∞‰∏ÄÊ¨° valid/test\n",
    "\n",
    "# for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "#     # ‚Äî‚Äî ËÆ≠ÁªÉ ‚Äî‚Äî \n",
    "#     tr_loss = train(model, device, train_loader, optimizer, criterion, scheduler=schedule)\n",
    "#     train_epoch_error.append(torch.as_tensor(tr_loss))\n",
    "#     train_check_idx.append(epoch + 1)\n",
    "\n",
    "#     # ‚Äî‚Äî ÊØè interval ‰∏™ epoch ÊâßË°å‰∏ÄÊ¨°È™åËØÅÂíåÊµãËØï ‚Äî‚Äî \n",
    "#     if (epoch + 1) % interval == 0:\n",
    "#         # ‚Äî‚Äî È™åËØÅ ‚Äî‚Äî \n",
    "#         val_loss = valid(model, device, valid_loader, criterion)\n",
    "#         val_epoch_error.append(torch.as_tensor(val_loss))\n",
    "\n",
    "#         # ‚Äî‚Äî ÊµãËØï ‚Äî‚Äî \n",
    "#         test_loss_list = []\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_loader:\n",
    "#                 batch = batch.to(device)\n",
    "#                 output = model(batch)\n",
    "#                 label = batch.y\n",
    "#                 loss = criterion(output, label)\n",
    "#                 test_loss_list.append(loss.item())\n",
    "#         avg_test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "#         test_epoch_error.append(avg_test_loss)\n",
    "\n",
    "#         # ‚Äî‚Äî ËÆ∞ÂΩïÂΩìÂâç step ‚Äî‚Äî \n",
    "#         valid_check_idx.append(epoch + 1)\n",
    "\n",
    "#         # ‚Äî‚Äî ‰øùÂ≠òÂΩìÂâçÊ®°ÂûãÔºà‰Ω†‰πüÂèØ‰ª•Âè™‰øùÂ≠òÊúÄ‰ºòÁöÑÔºâ ‚Äî‚Äî \n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'val_loss': val_loss,\n",
    "#             'test_loss': avg_test_loss\n",
    "#         }, f'checkpoint_epoch{epoch+1}.pt')\n",
    "\n",
    "\n",
    "# # ‚Äî‚Äî ÁªòÂõæ ‚Äî‚Äî \n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# # ËΩ¨Êç¢‰∏∫ numpy\n",
    "# train_loss_np = torch.stack(train_epoch_error).detach().cpu().numpy()\n",
    "# val_loss_np   = torch.stack(val_epoch_error).detach().cpu().numpy()\n",
    "# test_loss_np  = torch.tensor(test_epoch_error).detach().cpu().numpy()\n",
    "\n",
    "# # Áîª train lossÔºàÊØè‰∏ÄËΩÆÈÉΩÊúâÔºâ\n",
    "# ax.plot(train_check_idx, train_loss_np, label='L_train', linewidth=1)\n",
    "\n",
    "# # Áîª valid lossÔºàÊØè 200 ËΩÆÔºâ\n",
    "# ax.plot(valid_check_idx, val_loss_np, label='L_valid', linewidth=2, marker='o')\n",
    "\n",
    "# # Áîª test lossÔºàÊØè 200 ËΩÆÔºâ\n",
    "# ax.plot(valid_check_idx, test_loss_np, label='L_test', linewidth=2, marker='x')\n",
    "\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_title('Train / Valid / Test Loss Curve')\n",
    "# ax.set_xlabel('Epochs')\n",
    "# ax.set_ylabel('Loss')\n",
    "# ax.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cca7a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import spearmanr\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# per_point_diff_list = []\n",
    "# disp_list = []\n",
    "# mse_list = []\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# for batch in valid_loader: \n",
    "#     batch = batch.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         output = model(batch)\n",
    "#         label  = batch.y\n",
    "\n",
    "#         # ---- ÂèçÂΩí‰∏ÄÂåñ ----\n",
    "#         pressure_normalizer = Normalizer_ts(method=\"ms\",\n",
    "#                                             params=output_range[0, :2].reshape(2, 1))\n",
    "#         output_denorm = pressure_normalizer.denormalize(output.cpu()[:, 0])\n",
    "#         label_denorm  = pressure_normalizer.denormalize(label .cpu()[:, 0])\n",
    "\n",
    "#         # ---- ‰∏â‰∏™Ë°® 3 ÊåáÊ†á ----\n",
    "#         diff = output_denorm - label_denorm\n",
    "#         disp = torch.sum(diff ** 2) / torch.sum(output_denorm ** 2)   # Approx. Disp.\n",
    "#         mse  = torch.mean(diff ** 2)                                  # Test loss (MSE)\n",
    "\n",
    "#         per_point_diff_list.append(diff.cpu().numpy())\n",
    "#         disp_list.append(disp.item())\n",
    "#         mse_list.append(mse.item())\n",
    "#         all_preds.append(output_denorm.cpu().numpy())\n",
    "#         all_labels.append(label_denorm.cpu().numpy())\n",
    "\n",
    "# # ----------------- Ê±áÊÄªÂà∞Êï¥‰∏™È™åËØÅÈõÜ -----------------\n",
    "# preds   = np.concatenate(all_preds)\n",
    "# labels  = np.concatenate(all_labels)\n",
    "# diff_all = preds - labels\n",
    "\n",
    "# # ÂéüÊúâ 3 È°π\n",
    "# per_point_diff_mean = diff_all.mean()      # Per-point Diff.\n",
    "# disp_mean           = np.mean(disp_list)   # Approx. Disp.\n",
    "# mse_mean            = np.mean(mse_list)    # Test loss (MSE)\n",
    "\n",
    "# # # Êñ∞Â¢û œÅÔºàSpearmanÔºâ-------------------------------\n",
    "# # rho, _ = spearmanr(preds, labels)          # Ëã•Âè™ÁúãÁã≠Á™ÑÂå∫ÔºåÂèØÂÖàÁ≠õÈÄâÂÜçÁÆó\n",
    "\n",
    "# # # Êñ∞Â¢û Bias ------------------------------\n",
    "# # bias = diff_all.mean()\n",
    "\n",
    "\n",
    "# # ----------------- ÊâìÂç∞ -----------------\n",
    "# print(f\"{'Model':<12}\"\n",
    "#       f\"{'Test loss (Pa √ó 10‚Åª¬≤) ‚Üì':>20}\"\n",
    "#       f\"{'Per-point Diff. ‚Üì':>20}\"\n",
    "#       f\"{'Approx. Disp. ‚Üì':>18}\"\n",
    "#     #   f\"{'œÅ ‚Üë':>8}\"\n",
    "#     #   f\"{'Bias ‚Üì':>12}\"\n",
    "#       )\n",
    "# print('-' * 115)\n",
    "# print(f\"{'LaB-GATr':<12}\"\n",
    "#       f\"{mse_mean * 100:>20.2f}\"          # √ó10‚Åª¬≤ Áº©Êîæ\n",
    "#       f\"{per_point_diff_mean:>20.2E}\"\n",
    "#       f\"{disp_mean:>18.2E}\"\n",
    "#     #   f\"{rho:>8.3f}\"\n",
    "#     #   f\"{bias:>12.3E}\"\n",
    "#       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b078fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pyvista as pv\n",
    "# import matplotlib.pyplot as plt\n",
    "# from os.path import join as osj\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def generate_output(\n",
    "#     model,\n",
    "#     device,\n",
    "#     output_loader,\n",
    "#     raw_mesh_path,\n",
    "#     tag,\n",
    "#     save_mesh_path,\n",
    "#     mesh_name=\"mesh\",\n",
    "#     output_normalizer=None,\n",
    "# ):\n",
    "#     for _, batch in enumerate(tqdm(output_loader)):\n",
    "#         batch = batch.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             label = batch.y\n",
    "#             output = model(batch)\n",
    "#             label_denorm = output_normalizer.denormalize(label)  # shape is <N,1>\n",
    "#             output_denorm = output_normalizer.denormalize(output)  # shape is <N,1>\n",
    "#             idx = (batch.idx)[0]\n",
    "\n",
    "#             mesh = pv.read(osj(raw_mesh_path, \"aorta00{:d}.vtp\".format(idx)))\n",
    "#             mesh.point_data[tag + \"_prediction\"] = output_denorm.detach().cpu()\n",
    "#             mesh.point_data[tag + \"_label\"] = label_denorm.detach().cpu()\n",
    "#             mesh.point_data[tag + \"_error\"] = (\n",
    "#                 mesh.point_data[tag + \"_prediction\"] - mesh.point_data[tag + \"_label\"]\n",
    "#             )\n",
    "#             mesh.save(osj(save_mesh_path, mesh_name + \"_{:d}.vtp\".format(idx)))\n",
    "# generate_output(model_opt,'cpu',valid_loader,'aorta/raw','wss','results/meshes','mesh_wss_50',output_normalizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-gatr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
